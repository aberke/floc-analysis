{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions -> Domains\n",
    "\n",
    "Read data and convert to domain sets for each `machine_id, week` pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels = pd.read_csv(\"../output/all_panels.csv\", index_col=0)\n",
    "\n",
    "transactions_fpath = '../data/comscore/2017/transactions.csv'\n",
    "transactions_df = pd.read_csv(transactions_fpath, parse_dates=['event_date'])[['machine_id', 'event_date', 'domain_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = (transactions_df\n",
    " .assign(week=lambda x: x.event_date.dt.week)\n",
    " .dropna()\n",
    " .groupby(['machine_id', 'week'])\n",
    " .agg({'domain_name': set}).reset_index()\n",
    "       .assign(n_domains=lambda x: x.domain_name.map(len))\n",
    "       .assign(domain_name=lambda x: x.domain_name.map(lambda y: \"|\".join(list(y))))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.sort_values('n_domains', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('../output/domains_by_week.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Floc\n",
    "\n",
    "1. assign domain list history to machine IDs\n",
    "2. For each panel, calculate cohort assignment for a given week\n",
    "3. calculate t-closeness across stratified variables and other variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from floc import simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "unique_domains = list(set(itertools.chain(*[d.split(\"|\") for d in tdf.domain_name])))\n",
    "\n",
    "def pad_domains(l, s):\n",
    "    n = 7 - len(l)\n",
    "    if n > 0:\n",
    "        return list(l) + list(np.random.choice(s, n))\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "tdf['padded_domain'] = tdf.apply(lambda x: pad_domains(x.domain_name.split(\"|\"), unique_domains), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_domains = [d for d in tdf.padded_domain]\n",
    "cohorts = []\n",
    "for d in padded_domains:\n",
    "    cohorts.append(simulate(d, check_sensiveness=False))\n",
    "\n",
    "tdf['cohort'] = cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.cohort.value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that the existing clusters are far too small for our original t-closeness plan. But these cohort IDs are generated from the *actual* cohorts used in the OT, which resulted in ~30k cohorts. \n",
    "\n",
    "We found it surprising that we see representation from 18k cohorts from so few domains + people. \n",
    "\n",
    "We decided to move on to calculating our own cohorts from the data itself due to our smaller sample size. \n",
    "\n",
    "The argument remains the same: we want to test if FLoC violates reasonable t-closeness restrictions on demography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panels.groupby(['panel_id']).agg({'machine_id': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a first pass if cohorts are evenly distributed, how large will our cohorts be (from 1 week of data rather than multiple weeks as independent samples)?\n",
    "\n",
    "(this is the upper limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23670/50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems OK! For now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating SimHash & PrefixLSH\n",
    "\n",
    "from https://github.com/hybridtheory/floc-simhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to compute SimHash, and then run the 'prefixLSH' routine that splits each based on the 0/1 bit successively./ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sklearn floc_simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['padded_domain_string'] = tdf.padded_domain.map(lambda x: \"|\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from floc_simhash import SimHash\n",
    "hasher = SimHash(n_bits=50, tokenizer=lambda x: x.split(\"|\"))\n",
    "hashes = [hasher.hash(d) for d in tdf.padded_domain_string]\n",
    "tdf['simhash'] = hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv(\"../output/transaction_domain_simhash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.simhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have the simhash of each domain series (padded) in the transaction DF.\n",
    "\n",
    "Now to apply prefixLSH. How is it implemented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitarray import bitarray\n",
    "ba = bitarray()\n",
    "min_cluster_size = 50\n",
    "a = ba.frombytes(str.encode(hashes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hashes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.encode(hashes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "etlds = [requests.get(\"http://\" + d).url for d in unique_domains]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all \"blocked\" domains from FloC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_domains = list(set(itertools.chain(*blocked)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [[d] * 7 for d in unique_domains]\n",
    "blocked_domains = []\n",
    "for d in ds:\n",
    "    try:\n",
    "        simulate(d)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        blocked_domains.append(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socket.getfqdn('ww.' + blocked_domains[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(blocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.padded_domain[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_cluster_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['cohort'] = [simulate(domains, sorting_cluster_data) for domains in tdf.padded_domain]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-floc",
   "language": "python",
   "name": "conda-floc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
