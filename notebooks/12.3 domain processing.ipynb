{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import floc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sessions_fpath = '../data/comscore/2017/sessions.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all bad domains, save to `output/bad_domains.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "import floc\n",
    "df = pd.read_csv(sessions_fpath, usecols=['domain_name'], encoding=\"ISO-8859-1\")\n",
    "domains = df.domain_name.unique()\n",
    "\n",
    "def _is_bad_domain(domain): \n",
    "    try: \n",
    "        floc.hashes.sim_hash_string([domain])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return domain\n",
    "    \n",
    "def get_bad_domains(unique_domains, n_cores=2):\n",
    "    bad_domains = Parallel(n_jobs=n_cores)(delayed(_is_bad_domain)(d) for d in domains)\n",
    "    bad_domains = [d for d in bad_domains if d is not None]\n",
    "    return bad_domains\n",
    "\n",
    "bad_domains = get_bad_domains(domains, n_cores=24)\n",
    "\n",
    "with open('../output/bad_domains.txt', 'w') as f:\n",
    "    for domain in bad_domains:\n",
    "        f.write(str(domain) + \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_domains = []\n",
    "with open('../output/bad_domains.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        bad_domains.append(l.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in actual data, strip bad domains, format date correctly, and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map: {week: {machine_id: {set of domains from the week}}}\n",
    "#weeks_machines_\n",
    "machine_ids = pd.read_csv(sessions_fpath, usecols=['machine_id'], encoding=\"ISO-8859-1\").machine_id.unique()\n",
    "weeks_machines_domains = {w: {machine_id: set() for machine_id in machine_ids} for w in range(1, 52+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan1 = datetime.strptime(str('20170101'), '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(sessions_chunk):\n",
    "    # drop rows with nan or malformed domains\n",
    "    sessions_chunk.dropna(inplace=True)\n",
    "    # drop malformed domains\n",
    "    sessions_chunk = sessions_chunk[sessions_chunk.domain_name.map(lambda d: d not in bad_domains)]\n",
    "    # drop rows where event date is Jan 1 2017.\n",
    "    sessions_chunk = sessions_chunk[sessions_chunk.event_date > jan1]\n",
    "    # assign weeks\n",
    "    sessions_chunk['week'] = sessions_chunk.event_date.apply(lambda d: d.isocalendar()[1])\n",
    "    # group by week, machine_id so that each row corresponds to list of unique\n",
    "    # domains visited for given week for given machine\n",
    "    week_machine_domains = sessions_chunk.groupby(\n",
    "        ['week', 'machine_id']\n",
    "    )['domain_name'].unique()\n",
    "    return week_machine_domains\n",
    "\n",
    "n_cores = 10\n",
    "CHUNKSIZE=1000000\n",
    "chunks = pd.read_csv(sessions_fpath, \n",
    "                     encoding=\"ISO-8859-1\", \n",
    "                     chunksize=CHUNKSIZE, \n",
    "                     usecols=['machine_id', 'event_date', 'domain_name'])\n",
    "wmds = Parallel(n_jobs=n_cores)(delayed(process_chunk)(chunk) for chunk in chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reduce to set of week, machine_id -> [ unique domains ] after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(1, 52+1):\n",
    "    # maps for week w, {machine_id -> [array of unique domains]}\n",
    "    w_machine_domains_dict = week_machine_domains.xs(w).to_dict()\n",
    "    for m, d_array in w_machine_domains_dict.items():\n",
    "        weeks_machines_domains[w][m].update(set(d_array))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_machines_domains_df = pd.DataFrame({})\n",
    "for w in range(1, 52+1):\n",
    "    if w % 10 == 0:\n",
    "        print('week', w, datetime.now().time())\n",
    "    weeks_machines_domains_df = weeks_machines_domains_df.append(pd.DataFrame.from_dict(\n",
    "        # 12/3 note: we added the if len(d) > 0 to try and eliminate '' domains in the next step.\n",
    "        # check this once the chunk processing is working.\n",
    "        {m: [m, w, d, len(d)] for m,d in weeks_machines_domains[w].items() if len(d) > 0}, \n",
    "        orient='index', \n",
    "        columns=['machine_id', 'week','domains', 'n_domains']\n",
    "    ))\n",
    "weeks_machines_domains_df.drop('machine_id', axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:00:36.097913 : chunk 0\n",
      "0 rows --> 0 total rows\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2638190/502164791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# maps for week w, {machine_id -> [array of unique domains]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mw_machine_domains_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweek_machine_domains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_array\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_machine_domains_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mweeks_machines_domains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/floc-analysis/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3770\u001b[0;31m                 loc, new_index = index._get_loc_level(\n\u001b[0m\u001b[1;32m   3771\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3772\u001b[0m                 )\n",
      "\u001b[0;32m/opt/anaconda3/envs/floc-analysis/lib/python3.9/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   3110\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3112\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/floc-analysis/lib/python3.9/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc_single_level_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lexsort_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/floc-analysis/lib/python3.9/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_loc_single_level_index\u001b[0;34m(self, level_index, key)\u001b[0m\n\u001b[1;32m   2853\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlevel_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/floc-analysis/lib/python3.9/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;31m# unrecognized type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "jan1 = datetime.strptime(str('20170101'), '%Y%m%d')\n",
    "\n",
    "sessions_chunk = None\n",
    "total_rows = 0\n",
    "i = 0\n",
    "for sessions_chunk in pd.read_csv(sessions_fpath, \n",
    "                                  chunksize=CHUNKSIZE,\n",
    "                                  usecols=['machine_id', 'event_date', 'domain_name'],\n",
    "                                  parse_dates=['event_date'],\n",
    "                                  encoding=\"ISO-8859-1\"\n",
    "                                 ):\n",
    "    start_d = datetime.now()\n",
    "    print('%s : chunk %s' % (start_d.time(), i))\n",
    "    # drop rows with nan or malformed domains\n",
    "    sessions_chunk.dropna(inplace=True)\n",
    "    # drop malformed domains\n",
    "    sessions_chunk = sessions_chunk[sessions_chunk.domain_name.map(lambda d: d not in bad_domains)]\n",
    "    # drop rows where event date is Jan 1 2017.\n",
    "    sessions_chunk = sessions_chunk[sessions_chunk.event_date > jan1]\n",
    "    rows = len(sessions_chunk)\n",
    "    total_rows += rows\n",
    "    print('%s rows --> %s total rows' % (rows, total_rows))\n",
    "    #print('2 %s : chunk %s' % (datetime.now().time(), i))\n",
    "    # assign weeks\n",
    "    sessions_chunk['week'] = sessions_chunk.event_date.apply(lambda d: d.isocalendar()[1])\n",
    "    #print('3 %s : chunk %s' % (datetime.now().time(), i))\n",
    "    # group by week, machine_id so that each row corresponds to list of unique\n",
    "    # domains visited for given week for given machine\n",
    "    week_machine_domains = sessions_chunk.groupby(\n",
    "        ['week', 'machine_id']\n",
    "    )['domain_name'].unique()\n",
    "    #print('4 %s : chunk %s' % (datetime.now().time(), i))\n",
    "    # iterate over weeks to update the week_machines_domains dict for each week\n",
    "    for w in range(1, 52+1):\n",
    "        # maps for week w, {machine_id -> [array of unique domains]}\n",
    "        w_machine_domains_dict = week_machine_domains.xs(w).to_dict()\n",
    "        for m, d_array in w_machine_domains_dict.items():\n",
    "            weeks_machines_domains[w][m].update(set(d_array))    \n",
    "    i += 1\n",
    "#     if i > 1:\n",
    "#         break \n",
    "    print('time to handle chunk: %s' % (datetime.now() - start_d)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write df to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/comscore/2017/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_weeks_machines_domains(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/comscore/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_machines_domains_df = pd.DataFrame({})\n",
    "for w in range(1, 52+1):\n",
    "    if w % 10 == 0:\n",
    "        print('week', w, datetime.now().time())\n",
    "    weeks_machines_domains_df = weeks_machines_domains_df.append(pd.DataFrame.from_dict(\n",
    "        {m: [m, w, d, len(d)] for m,d in weeks_machines_domains[w].items()}, \n",
    "        orient='index', \n",
    "        columns=['machine_id', 'week','domains', 'n_domains']\n",
    "    ))\n",
    "weeks_machines_domains_df.drop('machine_id', axis=1).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-floc",
   "language": "python",
   "name": "conda-floc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
